{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tom O'Leary-Roseberry\n",
    "## Homework 3\n",
    "\n",
    "### 2 Programming\n",
    "\n",
    "We are interested in solving the following low rank matrix problem. Given a sparse observation pattern $G \\in \\{0,1\\}^{n\\times n}$ and a data matrix $A \\in \\mathbb{R}^n$ our goal is to recover a low rank matrix pair, $B \\in \\mathbb{R}^{n\\times r}, C \\in \\mathbb{R}^{r \\times n}$ by minimizing\n",
    "\n",
    "$\\min\\limits_{B,C} f(B,C) =  \\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n g_{ij}(a_{ij} - e_i^TB(Ce_j))^2 + \\frac{\\mu}{2}(\\|B\\|_F^2 + \\|C\\|_F^2) $\n",
    "\n",
    "Note that this is the same as:\n",
    "\n",
    "$\\min\\limits_{B,C} \\big\\langle G, (A -BC)\\circ(A-BC)\\big\\rangle_F + \\frac{\\mu}{2} \\big\\langle B, B \\big\\rangle_F + \\frac{\\mu}{2} \\big\\langle C,C\\big\\rangle_F$\n",
    "\n",
    "Where $\\circ$ is the Hadamard product, and $\\langle \\cdot ,\\cdot\\rangle_F$ is the Frobenius inner product.\n",
    "\n",
    "Deriving the gradient is an exercise in tensor calculus:\n",
    "\n",
    "$g(f) = \\left[\\begin{array}{cc} \\nabla_Bf ,\\nabla_Cf\\end{array} \\right]$\n",
    "\n",
    "and $\\nabla_Bf \\in \\mathbb{R}^{n\\times r}$ likewise $\\nabla_Cf \\in \\mathbb{R}^{r\\times n}$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial b_{lm}} = \\frac{\\partial}{\\partial b_{lm}}\\bigg(\\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n g_{ij}(a_{ij} - \\sum\\limits_{k=1}^r b_{ik}c_{kj})(a_{ij} - \\sum\\limits_{t=1}^r b_{it}c_{tj}) + \\frac{\\mu}{2}\\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^n(b_{ik}b_{ik} + c_{ik}c_{ik})\\bigg) $\n",
    "\n",
    "I will first focus on the cost or least squares portion:\n",
    "\n",
    "$\\frac{\\partial}{\\partial b_{lm}}\\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n g_{ij}(a_{ij} - \\sum\\limits_{k=1}^r b_{ik}c_{kj})(a_{ij} - \\sum\\limits_{t=1}^r b_{it}c_{tj})\\\\\n",
    "= \\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n g_{ij}( - \\sum\\limits_{k=1}^r \\delta_{il}\\delta_{km}c_{kj})(a_{ij} - \\sum\\limits_{t=1}^r b_{it}c_{tj})+ \\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n g_{ij}(a_{ij} - \\sum\\limits_{k=1}^r b_{ik}c_{kj})(- \\sum\\limits_{t=1}^r \\delta_{il}\\delta_{tm}c_{tj}) $\n",
    "\n",
    "$= \\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n g_{ij}( - \\delta_{il}c_{mj})(a_{ij} - \\sum\\limits_{t=1}^r b_{it}c_{tj})+ \\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n g_{ij}(a_{ij} - \\sum\\limits_{k=1}^r b_{ik}c_{kj})(-  \\delta_{il}c_{mj}) $\n",
    "\n",
    "$= -2 \\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n\\delta_{il}g_{ij}(a_{ij} - \\sum\\limits_{k=1}^r b_{ik}c_{kj})c_{mj} $\n",
    "\n",
    "$= -2 \\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n\\delta_{il}g_{ij}(A-BC)_{ij}c_{mj} $\n",
    "\n",
    "$= -2 \\sum\\limits_{j=1}^ng_{lj}(A-BC)_{lj}c_{mj} $\n",
    "\n",
    "$= -2 \\sum\\limits_{j=1}^n\\big(G \\circ(A-BC)\\big)_{lj}c_{mj} $\n",
    "\n",
    "$= -2 \\bigg(\\big(G \\circ(A-BC)\\big)C^T\\bigg)_{lm} $\n",
    "\n",
    "A derivative with respect to the regulation yields\n",
    "\n",
    "$\\frac{\\partial}{\\partial b_{lm}}(b_{ik}b_{ik})  = 2 b_{ik}\\delta_{il}\\delta_{km} = 2b_{lm}$\n",
    "\n",
    "\n",
    "The whole gradient can be represented in matrix form as:\n",
    "\n",
    "$\\nabla_B f = -2 \\big(G\\circ(A - BC)\\big)C^T + \\mu B \\in \\mathbb{R}^{n\\times r}$\n",
    "\n",
    "\n",
    "Similarly for the gradient with respect to $C$, I derived it as:\n",
    "\n",
    "$\\nabla_C f = -2B^T\\big(G \\circ(A-BC) \\big) + \\mu C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The Hessian action is needed at the very least to implement a trust region algorithm. The Hessian is a fourth order tensor, its action on a second order matrix results in a second order matrix (as is expected). Heuristically I will decompose the Hessian into a block structure as follows:\n",
    "\n",
    "$H = \\left[\\begin{array}{cc} H_{BB} & H_{BC} \\\\ H_{CB} & H_{CC} \\end{array}\\right] $\n",
    "\n",
    "each block has $n^2r^2$ elements but the shapes vary as $B$ and $C$ have different shapes. Careful attention to indices is warranted.\n",
    "\n",
    "$(H_{BB})_{lmpq} = \\frac{\\partial}{\\partial b_{pq}} (\\nabla_B f)_{lm} = 2 \\sum\\limits_{j=1}^n g_{lj}\\delta_{lp}c_{qj}c_{mj} + \\mu \\delta_{lp}\\delta_{kq}$\n",
    "\n",
    "$(H_{BC})_{lmpq} = \\frac{\\partial}{\\partial c_{qp}} (\\nabla_B f)_{lm} = 2g_{lp}b_{lq}c_{mp} -2(G\\circ (A-BC))_{lp} \\delta_{mq}$\n",
    "\n",
    "$(H_{CB})_{lmpq} = \\frac{\\partial}{\\partial b_{pq}} (\\nabla_C f)_{lm} = 2g_{pl}b_{pm}c_{ql} -2(G\\circ (A-BC))_{pl} \\delta_{mq}$\n",
    "\n",
    "$(H_{CC})_{lmpq} = \\frac{\\partial}{\\partial c_{qp}} (\\nabla_C f)_{lm} = 2\\sum\\limits_{i=1}^n b_{im}g_{il} b_{iq}\\delta_{lp} + \\mu \\delta_{mq}\\delta_{lp}$\n",
    "\n",
    "In order to implement a trust region algorithm we only need the Hessian action (we can also do Newton with just the action if we use a Krylov solver).\n",
    "\n",
    "The Hessian Action can be defined as follows:\n",
    "\n",
    "$\\left[\\begin{array}{cc} H_{BB} & H_{BC} \\\\ H_{CB} & H_{CC} \\end{array}\\right] \\left[\\begin{array}{c}X \\\\ Y  \\end{array} \\right] = \\left[\\begin{array}{c}H_{BB}X + H_{BC}Y\\\\H_{CB} X + H_{CC}Y \\end{array} \\right]$\n",
    "\n",
    "Each block multiplication is as follows:\n",
    "\n",
    "$H_{BB} X = [G \\circ (XC)]C^T + \\mu X$\n",
    "\n",
    "$H_{BC} Y = 2C[ G \\circ (BY)]^T - 2Y[G\\circ (A-BC)]^T$\n",
    "\n",
    "$H_{CB} X = 2[G \\circ (XC)]^T B - 2[G \\circ (A-BC)]^T X$\n",
    "\n",
    "$H_{CC} Y = B^T[G\\circ(BY)] + \\mu Y$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "I will now derive a class for this problem that will encapsulate all the important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "class low_rank_matrix_handler:\n",
    "    def __init__(self,A,B,C,G):\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.G = G\n",
    "        self.mu = 1.0\n",
    "        self.n,self.r = self.B.shape\n",
    "    \n",
    "    def control(self):\n",
    "        return np.array([self.B,self.C])\n",
    "    \n",
    "    def objective(self,B=None,C=None):\n",
    "        if B is None:\n",
    "            B = self.B\n",
    "        if C is None:\n",
    "            C = self.C\n",
    "        temp = self.A - np.matmul(B,C)\n",
    "        temp = np.multiply(temp,temp)\n",
    "        cost = np.sum(np.multiply(self.G,temp))\n",
    "        reg_B = 0.5*self.mu*np.sum(np.multiply(B,B))\n",
    "        reg_C = 0.5*self.mu*np.sum(np.multiply(C,C))\n",
    "        return cost + reg_B + reg_C\n",
    "    \n",
    "    def gradient(self):\n",
    "        # Initialize to the regularization portion\n",
    "        reg_B = self.mu*self.B \n",
    "        reg_C = self.mu*self.C\n",
    "        # Add the least squares (cost) portion\n",
    "        A_BC = self.A - np.matmul(self.B,self.C)\n",
    "        G_had_A_BC = np.multiply(self.G,A_BC)\n",
    "        cost_B = -2*np.matmul(G_had_A_BC,self.C.T)\n",
    "        cost_C = -2*np.matmul(self.B.T,G_had_A_BC)\n",
    "        grad_B = cost_B + reg_B\n",
    "        grad_C = cost_C + reg_C\n",
    "        return grad_B, grad_C\n",
    "    \n",
    "    def Hessian_action(self, X = None, Y=None):\n",
    "        if X is None:\n",
    "            X = np.zeros_like(self.B)\n",
    "        if Y is None:\n",
    "            Y = np.zeros_like(self.C)\n",
    "        shape_B = self.B.shape\n",
    "        shape_C = self.C.shape\n",
    "        if X.shape != shape_B:\n",
    "            raise ValueError(X)\n",
    "        if Y.shape != shape_C:\n",
    "            raise ValueError(Y)\n",
    "        XC = np.matmul(X,self.C)\n",
    "        G_had_XC = np.multiply(self.G,XC)\n",
    "        H_BB_X = np.matmul(G_had_XC,self.C.T) + self.mu*X\n",
    "        A_BC = self.A - np.matmul(self.B,self.C)\n",
    "        BY = np.matmul(self.B,Y)\n",
    "        G_had_BY = np.multiply(self.G,BY)\n",
    "        G_had_A_BC = np.multiply(self.G,A_BC)\n",
    "        H_BC_Y = 2*np.matmul(self.C,G_had_BY.T) - 2*np.matmul(Y,G_had_A_BC.T)\n",
    "        H_CB_X = 2*np.matmul(G_had_XC.T,self.B) - 2*np.matmul(G_had_A_BC.T,X)\n",
    "        H_CC_Y = np.matmul(self.B.T,G_had_BY) + self.mu*Y\n",
    "        return [H_BB_X+H_BC_Y.T,H_CB_X+ H_CC_Y.T]\n",
    "    \n",
    "    def Hessian_inner(self, X = None,Y=None):\n",
    "        if X is None:\n",
    "            X = np.zeros_like(self.B)\n",
    "        if Y is None:\n",
    "            Y = np.zeros_like(self.C)\n",
    "        H_XY = self.Hessian_action(X,Y)\n",
    "        H_X = H_XY[0]\n",
    "        H_Y = H_XY[1]\n",
    "        X_H_X = np.multiply(X,H_X)\n",
    "        Y_H_Y = np.multiply(Y.T,H_Y)\n",
    "        return np.sum(X_H_X) + np.sum(Y_H_Y)\n",
    "    \n",
    "    def update_control(self, P_B= None,P_C = None,alpha = 1.):\n",
    "        if P_B == None:\n",
    "            P_B = np.zeros_like(self.B)\n",
    "        if P_C == None:\n",
    "            P_C = np.zeros_like(self.C)\n",
    "        self.B += alpha*P_B\n",
    "        self.C += alpha*P_C\n",
    "        \n",
    "    def assemble_flat_Hessian(self):\n",
    "        n,r = self.B.shape\n",
    "        dim = n*r\n",
    "        H_BB = np.zeros((dim,dim))\n",
    "        H_BC = np.zeros((dim,dim))\n",
    "        H_CB = np.zeros((dim,dim))\n",
    "        H_CC = np.zeros((dim,dim))\n",
    "        A_BC = self.A - np.matmul(self.B,self.C)\n",
    "        G_had_A_BC = np.multiply(self.G,A_BC)\n",
    "        for l in range(n):\n",
    "            for m in range(r):\n",
    "                for p in range(n):\n",
    "                    for q in range(r):\n",
    "\n",
    "                        H_BB[l + n*m][p + n*q] += self.mu*(l==p)*(m==q)\n",
    "                        H_BC[l + n*m][p + n*q] += 2*self.G[l,p]*self.B[l,q]*self.C[m,p]\\\n",
    "                            -2*G_had_A_BC[l,p]*(m == q)\n",
    "                        H_CB[l + n*m][p + n*q] += 2*self.G[p,l]*self.B[p,m]*self.C[q,l]\\\n",
    "                            - 2*G_had_A_BC[p,l]*(m == q)\n",
    "                        H_CC[l + n*m][p + n*q] += self.mu*(l==p)*(m==q)\n",
    "                        for j in range(n):\n",
    "                            H_BB[l + n*m][p + n*q] += 2*self.G[l,j]*(l==p)\\\n",
    "                            *self.C[q,j]*self.C[m,j]\n",
    "                            H_CC[l + n*m][p + n*q] += 2*self.G[j,l]*(l==p)\\\n",
    "                            *self.B[j,m]*self.B[j,q]\n",
    "        H = np.bmat([[H_BB,H_BC],[H_CB,H_CC]])\n",
    "        return H\n",
    "        \n",
    "    def assemble_flat_gradient(self):\n",
    "        n,r = self.B.shape\n",
    "        dim = n*r\n",
    "        flat_grad_B = np.zeros(dim)\n",
    "        flat_grad_C = np.zeros(dim)\n",
    "        A_BC = self.A - np.matmul(self.B,self.C)\n",
    "        G_had_A_BC = np.multiply(self.G,A_BC)\n",
    "        for l in range(n):\n",
    "            for m in range(r):\n",
    "                flat_grad_B[l+n*m] += self.mu*self.B[l,m]\n",
    "                flat_grad_C[l+n*m] += self.mu*self.C[m,l]\n",
    "                for j in range(n):\n",
    "                    flat_grad_B[l+n*m] += -2*G_had_A_BC[l,j]*self.C[m,j]\n",
    "                    flat_grad_C[l+n*m] += -2*G_had_A_BC[j,l]*self.B[j,m]\n",
    "        g = np.concatenate([flat_grad_B,flat_grad_C])\n",
    "        return g\n",
    "    \n",
    "    def split_and_reshape(self,p):\n",
    "        temp_B,temp_C = np.split(p,2)\n",
    "        p_B = temp_B.reshape((self.n,self.r))\n",
    "        p_C = temp_C.reshape((self.n,self.r))\n",
    "        return p_B, p_C.T\n",
    "    \n",
    "    def predicted_reduction(self,p):\n",
    "        g = self.gradient()\n",
    "        gp = np.sum(np.multiply(g[0],p[0]))\n",
    "        pHp = self.Hessian_inner(X = p[0],Y=p[1])\n",
    "        return -gp -0.5*pHp\n",
    "    \n",
    "    def tensor_norm(self,p):\n",
    "        p_norm_2 = np.sum(np.multiply(p[0],p[0])) +np.sum( np.multiply(p[1],p[1])) \n",
    "        return np.sqrt(p_norm_2)\n",
    "        \n",
    "    def tensor_rescale(self,p,alpha):\n",
    "        return [alpha*p[0],alpha*p[1]]\n",
    "    \n",
    "    def assemble_tensor_Hessian(self):\n",
    "        n,r = self.B.shape\n",
    "        dim = n*r\n",
    "        H = np.zeros((2*n,r,2*n,r))\n",
    "        A_BC = self.A - np.matmul(self.B,self.C)\n",
    "        G_had_A_BC = np.multiply(self.G,A_BC)\n",
    "        for l in range(n):\n",
    "            for m in range(r):\n",
    "                for p in range(n):\n",
    "                    for q in range(r):\n",
    "                        #HBB\n",
    "                        H[l,m,p,q] += self.mu*(l==p)*(m==q)\n",
    "                        #HBC\n",
    "                        H[n+l,m,p,q] += 2*self.G[l,p]*self.B[l,q]*self.C[m,p]\\\n",
    "                            -2*G_had_A_BC[l,p]*(m == q)\n",
    "                        #HCB\n",
    "                        H[l,m,n+p,q] += 2*self.G[p,l]*self.B[p,m]*self.C[q,l]\\\n",
    "                            - 2*G_had_A_BC[p,l]*(m == q)\n",
    "                        #HCC\n",
    "                        H[n+l,m,n+p,q] += self.mu*(l==p)*(m==q)\n",
    "                        for j in range(n):\n",
    "                            #HBB\n",
    "                            H[l,m,p,q] += 2*self.G[l,j]*(l==p)\\\n",
    "                            *self.C[q,j]*self.C[m,j]\n",
    "                            #HCC\n",
    "                            H[n+l,m,n+p,q] += 2*self.G[j,l]*(l==p)\\\n",
    "                            *self.B[j,m]*self.B[j,q]\n",
    "        return H\n",
    "        \n",
    "    def assemble_tensor_gradient(self):\n",
    "        n,r = self.B.shape\n",
    "        gradient = np.zeros((2*n,r))\n",
    "        A_BC = self.A - np.matmul(self.B,self.C)\n",
    "        G_had_A_BC = np.multiply(self.G,A_BC)\n",
    "        for l in range(n):\n",
    "            for m in range(r):\n",
    "                #G_B\n",
    "                gradient[l,m] += self.mu*self.B[l,m]\n",
    "                #G_C\n",
    "                gradient[n+l,m] += self.mu*self.C[m,l]\n",
    "                for j in range(n):\n",
    "                    #G_B\n",
    "                    gradient[l,m] += -2*G_had_A_BC[l,j]*self.C[m,j]\n",
    "                    #G_C\n",
    "                    gradient[n+l,m] += -2*G_had_A_BC[j,l]*self.B[j,m]\n",
    "        return gradient\n",
    "    \n",
    "    def split_p(self,p):\n",
    "        p_B,p_CT = np.split(p,2)\n",
    "        return p_B, p_CT.T\n",
    "        \n",
    "    def Newton_direction_tensor(self):\n",
    "        g = self.assemble_tensor_gradient()\n",
    "        H = self.assemble_tensor_Hessian()\n",
    "        p = np.linalg.tensorsolve(H,-g)\n",
    "        return self.split_p(p)\n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to generate the data matrix $A$, sparse observational pattern $G$ and initialize the control variables $B$ and $C$\n",
    "\n",
    "\n",
    "Because it doesn't really matter I will take $A \\sim \\mathcal{N}(0,\\sigma^2)^{n \\times n}$\n",
    "\n",
    "I take $B$ and $C$ to be a pair of low rank recovery of identity (i.e. they are both identity padded with extra zeros in the strictly rectuangular rows or columns see $\\texttt{numpy.matlib.eye}$ for implementation). And $B*C = I_r \\in \\mathbb{R}^{n\\times n}$, identity up to the $r^{th}$ diagonal and then all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8\n",
    "r = 4\n",
    "mean = 0.0\n",
    "sigma = 1.0\n",
    "from numpy.matlib import eye\n",
    "B = eye(n=n,M=r)\n",
    "C = eye(n=r,M=n)\n",
    "\n",
    "\n",
    "A = np.matmul(B,C)\n",
    "\n",
    "B += np.random.normal(loc=mean,scale=sigma,size=(n,r))\n",
    "C += np.random.normal(loc=mean,scale=sigma,size=(r,n))\n",
    "G = np.multiply(np.random.randint(2,size=(n,n)),np.random.randint(2,size=(n,n)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternating_minimization(low_rank_matrix_handler,tolerance = 1e-8,max_iters =10000,\n",
    "                     c_armijo = 1e-4,rho_armijo =0.9,verbose = False,\n",
    "                     back_track = False,min_length = 1e-3):\n",
    "    lagr = low_rank_matrix_handler\n",
    "    grad_norm0_sq = la.norm(lagr.gradient()[0])**2 + la.norm(lagr.gradient()[1])**2\n",
    "    grad_norm0 = np.sqrt(grad_norm0_sq)\n",
    "    grad_norm = grad_norm0\n",
    "    iters = 0\n",
    "    \n",
    "    while iters <= max_iters:\n",
    "        for i in range(2):\n",
    "            partial_grad_norm0 = la.norm(lagr.gradient()[i])\n",
    "            partial_grad_norm = partial_grad_norm0\n",
    "            while partial_grad_norm > tolerance*partial_grad_norm0:\n",
    "                cost = lagr.objective()\n",
    "                P = lagr.gradient()\n",
    "                alpha = 0.1\n",
    "                if back_track:\n",
    "                    cost_old = cost\n",
    "                    alpha = 1.\n",
    "                    while alpha > min_length:\n",
    "                        B_test = lagr.B -alpha*P[0]\n",
    "                        C_test = lagr.C -alpha*P[1]\n",
    "                        if i ==0:\n",
    "                            cost = lagr.objective(B_test)\n",
    "                        elif i == 1:\n",
    "                            cost = lagr.objective(C_test)\n",
    "                        if cost < cost_old +c_armijo*alpha*grad_norm :\n",
    "                            break\n",
    "                        else:\n",
    "                            alpha *= rho_armijo\n",
    "                if i == 0:\n",
    "                    lagr.B -= alpha*P[0]\n",
    "                elif i == 1:\n",
    "                    lagr.C -= alpha*P[1]\n",
    "                partial_grad_norm = la.norm(P[i])\n",
    "                grad_norm = lagr.tensor_norm(P)\n",
    "                if (grad_norm < tolerance*grad_norm0):\n",
    "                    return iters\n",
    "                if verbose:\n",
    "                    alt_string = ['B','C']\n",
    "                    if iters == 0:\n",
    "                        print \"\\n{0:5} {1:5} {2:15} {3:15} {4:20} {5:15}\".format(\n",
    "                              \"Iter\",'Alt' , \"cost\", \"||g||L2\",\"partial \\|g\\|L2\", \"alpha\" )\n",
    "                    else:\n",
    "                        print \"\\n{0:<5d} {1:<5} {2:<15e} {3:<15e} {4:<20e} {5:<15e}\".format(\n",
    "                              iters, alt_string[i], cost, grad_norm, partial_grad_norm, alpha )\n",
    "                converged = (grad_norm < tolerance*grad_norm0)\n",
    "                if converged:\n",
    "                    return converged, iters\n",
    "                iters += 1\n",
    "    return converged, iters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c32966a2891e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlagr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlow_rank_matrix_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "B = 1e-1*np.ones((n,r))\n",
    "C = 1e-1*np.ones((r,n))\n",
    "\n",
    "lagr = low_rank_matrix_handler(A,B,C,G)\n",
    "\n",
    "\n",
    "g = lagr.gradient()\n",
    "\n",
    "alternating_minimization(lagr,back_track = False,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Newton(low_rank_matrix_handler,tolerance = 1e-8,max_iters =10000,\n",
    "                     c_armijo = 1e-4,rho_armijo =0.9,verbose = False,\n",
    "                     back_track = False,min_length = 1e-3):\n",
    "    lagr = low_rank_matrix_handler\n",
    "    grad_norm0_sq = la.norm(lagr.gradient()[0])**2 + la.norm(lagr.gradient()[1])**2\n",
    "    grad_norm0 = np.sqrt(grad_norm0_sq)\n",
    "    grad_norm = grad_norm0\n",
    "    iters = 0\n",
    "    \n",
    "    \n",
    "    while grad_norm > tolerance*grad_norm0 and iters <= max_iters:\n",
    "        cost = lagr.objective()\n",
    "        g = lagr.assemble_flat_gradient()\n",
    "        H = lagr.assemble_flat_Hessian()\n",
    "        P = lagr.Newton_direction_tensor()\n",
    "        alpha = 1.0\n",
    "        if back_track:\n",
    "            cost_old = cost\n",
    "            alpha = 1.\n",
    "            while alpha > min_length:\n",
    "                B_test = lagr.B +alpha*P[0]\n",
    "                C_test = lagr.C +alpha*P[1]\n",
    "                cost = lagr.objective(B_test,C_test)\n",
    "                if cost < cost_old +c_armijo*alpha*grad_norm :\n",
    "                    break\n",
    "                else:\n",
    "                    alpha *= rho_armijo\n",
    "        lagr.B += alpha*P[0]\n",
    "        lagr.C += alpha*P[1]\n",
    "        grad_norm = lagr.tensor_norm(P)\n",
    "        if verbose:\n",
    "            if iters == 0:\n",
    "                print \"\\n{0:10} {1:15} {2:15} {3:15}\".format(\n",
    "                      \"Iteration\",  \"cost\", \"||g||L2\", \"alpha\" )\n",
    "            else:\n",
    "                print \"\\n{0:<10d} {1:<15e} {2:<15e} {3:<15e}\".format(\n",
    "                      iters,  cost, grad_norm, alpha )\n",
    "        converged = (grad_norm < tolerance*grad_norm0)\n",
    "        if converged:\n",
    "            return converged, iters\n",
    "        iters += 1\n",
    "    return converged, iters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration  cost            ||g||L2         alpha          \n",
      "\n",
      "1          2.728845e+01    5.305739e+00    1.000000e+00   \n",
      "\n",
      "2          3.351323e+00    1.003696e+01    1.000000e+00   \n",
      "\n",
      "3          1.151980e+03    9.298092e+00    1.000000e+00   \n",
      "\n",
      "4          9.234720e+00    7.417011e+00    1.000000e+00   \n",
      "\n",
      "5          4.058713e+01    2.748416e+00    1.000000e+00   \n",
      "\n",
      "6          1.070959e+01    1.622058e+00    1.000000e+00   \n",
      "\n",
      "7          3.297881e+00    2.959304e+00    1.000000e+00   \n",
      "\n",
      "8          1.710127e+00    7.327686e-01    1.000000e+00   \n",
      "\n",
      "9          1.135522e+00    8.775151e-01    1.000000e+00   \n",
      "\n",
      "10         7.506281e-01    3.544399e-02    1.000000e+00   \n",
      "\n",
      "11         7.500000e-01    3.188247e-06    1.000000e+00   \n",
      "\n",
      "12         7.500000e-01    2.320439e-18    1.000000e+00   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.ones((n,r))\n",
    "C = np.ones((r,n))\n",
    "\n",
    "lagr = low_rank_matrix_handler(A,B,C,G)\n",
    "\n",
    "Newton(lagr,back_track = False,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trust_region(low_rank_matrix_handler,tolerance = 1e-3,max_iters =10000,\n",
    "                     delta_hat = 1.,delta_0_rel = 1.0, eta = 0.25,verbose = False):\n",
    "    lagr = low_rank_matrix_handler\n",
    "    grad_norm0 = lagr.tensor_norm(lagr.gradient())\n",
    "    grad_norm = grad_norm0\n",
    "    iters = 0\n",
    "    delta_0 = delta_0_rel*delta_hat\n",
    "    delta = delta_0\n",
    "    while grad_norm > tolerance*grad_norm0 and iters <= max_iters:\n",
    "        cost_0 = lagr.objective()\n",
    "        P = lagr.Newton_direction_tensor()\n",
    "        P_norm = lagr.tensor_norm(P)\n",
    "        if P_norm > delta:\n",
    "            scale = delta/P_norm\n",
    "            P = lagr.tensor_rescale(P,scale)\n",
    "            P_norm = delta\n",
    "        predicted_reduction = lagr.predicted_reduction(P)\n",
    "        B_test = lagr.B +P[0]\n",
    "        C_test = lagr.C +P[1]\n",
    "        cost_p = lagr.objective(B=B_test,C=C_test) \n",
    "        actual_reduction = cost_0 - cost_p\n",
    "        rho = actual_reduction/predicted_reduction\n",
    "        if rho < 0.25:\n",
    "            delta *= 0.25\n",
    "        else:\n",
    "            if (rho > 0.75) and (P_norm == delta):\n",
    "                delta = min(2*delta,delta_hat)\n",
    "        if rho > eta:\n",
    "            lagr.B += delta*P[0]\n",
    "            lagr.C += delta*P[1]\n",
    "        grad_norm = lagr.tensor_norm(lagr.gradient())       \n",
    "        if verbose:\n",
    "            if iters == 0:\n",
    "                print \"\\n{0:4} {1:9} {2:9} {3:9} {4:9}\".format(\n",
    "                      \"Iteration\",  \"cost\", \"||g||L2\", \"rho\", \"delta\" )\n",
    "            else:\n",
    "                print \"\\n{0:<9d} {1:<9.1e} {2:<9.1e} {3:<9.1e} {4:<9.1e}\".format(\n",
    "                      iters,  cost_0, grad_norm, rho, delta )\n",
    "        converged = (grad_norm < tolerance*grad_norm0)\n",
    "        if converged:\n",
    "            return converged, iters\n",
    "        if (rho ==np.nan):\n",
    "            return False, iters\n",
    "        iters += 1\n",
    "    return iters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "B = 1e-1*np.ones((n,r))\n",
    "C = 1e-1*np.ones((r,n))\n",
    "\n",
    "lagr = low_rank_matrix_handler(A,B,C,G)\n",
    "\n",
    "trust_region(lagr,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "I would like to note that each iteration of a full on Newton scales with $n^6$, which to me seems unreal. Quasi Newton methods will have reduction but still forming tensors (or flattened matrices) that scale with $n^{2\\text{ linear algebra order}}$ seems crazy for a large scale problem like this.\n",
    "\n",
    "Why wouldn't someone just use a randomized SVD for this type of problem? This seems like the wrong tool for the job a priori due to the prohibitive work order (even for gradient descent).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
