{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tom O'Leary-Roseberry\n",
    "## Homework 4\n",
    "\n",
    "### 2 Programming\n",
    "\n",
    "We are interested in solving:\n",
    "\n",
    "$\\min\\limits_x \\sum\\limits_{i=1}^m \\|A_ix_i - b_i\\|^2 + \\lambda \\sum\\limits_{i=1}^m \\sum\\limits_{j=i+1}^m \\|x_i - x_j\\|_1$\n",
    "\n",
    "Using proximal gradient methods using some proximal gradient method and generated data set.\n",
    "\n",
    "Let $A_i \\in \\mathbb{R}^{n\\times k}$ and $b_i \\in \\mathbb{R}^n$, so $x_i \\in \\mathbb{R}^k$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to think of $\\{x_i\\}_{i=1}^m = X$ as a data matrix $X \\in \\mathbb{R}^{m \\times k}$ based on how it imports from the matlab matrix file.\n",
    "\n",
    "\n",
    "At a given iteration of proximal gradients, we compute the gradient with respect to the $C^1$ portion of the objective function\n",
    "\n",
    "$\\sum\\limits_{i=1}^m \\|A_ix_i - b_i\\|^2$\n",
    "\n",
    "and then filter with respect to the $C^0$ sparsifying penalty term \n",
    "\n",
    "$\\sum\\limits_{i=1}^m \\sum\\limits_{j=i+1}^m \\|x_i - x_j\\|_1$\n",
    "\n",
    "The filtering process amounts to the derivation of the proximity operator, which for a function $\\phi$ is defined as follows:\n",
    "\n",
    "$\\text{prox}_\\phi = \\text{arg}\\min\\limits_{x \\in \\mathcal{H}} \\phi(x) + \\frac{1}{2}\\|u - x\\|^2_2$\n",
    "\n",
    "The algorithm for calculating the proximity operator is defined as follows for this problem:\n",
    "\n",
    "1. \n",
    "    a. At a given iteration each row of $X$ is sorted $(x_0,x_1,\\dots, x_m)$ such that $x_0 \\leq x_1 \\leq \\dots \\leq x_m$\n",
    "    \n",
    "    b. In order to account for degenerate cases where two coordinates are equal the list is collapsed into a list of tuples:\n",
    "    \n",
    "    $\\{ (x_i,l_i) | l_i = \\#\\{x_i \\text{in} (x_0,x_1,\\dots, x_m)\\} \\}$\n",
    "    \n",
    "    let the length of this list be $n$\n",
    "    \n",
    "2. The problem can now be posed as the following minimization problem\n",
    "\n",
    "    $u = \\text{arg}\\min\\limits_{u_i}  \\sum\\limits_{i=1}^n \\sum\\limits_{j=i+1}^n \\lambda l_i l_j(u_i - u_j) + \\sum\\limits_{i=1}^n \\frac{l_i}{2} |u_i - x_i|^2$\n",
    "\n",
    "    Note that an immediate consequence is the property that if $u$ is a minimizer of this problem then $u_0 \\leq u_1 \\leq\\dots $\n",
    "\n",
    "    If this were not the case at some point in the list the ordering was not monotonic then there would be one term in the first term of the objective function that would increase the cost relative to the ordering being flipped. This is a contraction and the list must be monotonic.\n",
    "\n",
    "3. We can then pose the equivalent saddle point problem with Lagrange multiplier $\\mu$ that penalizes deviations from this optimal property\n",
    "\n",
    "    $\\min\\limits_{u_i}\\max\\limits_{\\mu_i\\geq 0} \\mathcal{L}(u,\\mu) =\\min\\limits_{u_i}\\max\\limits_{\\mu_i\\geq 0}\\sum_k \\mu_k(u_k - u_{k+1}) + \\sum\\limits_{i<j}l_il_j(u_j - u_i) + \\sum\\limits_i \\frac{l_i}{2}|u_i - x_i|^2 $\n",
    "\n",
    "    Where the counter $l$ allows us to collapse entire sums by counting up the number of redudant terms with $l_i l_j$ etc.\n",
    "    \n",
    "4. Next define the coefficient:\n",
    "\n",
    "    $ c_k = \\lambda \\bigg(\\sum_{j>k} l_j - \\sum_{k>j}l_j \\bigg) + x_k$\n",
    "    \n",
    "    The stationary points of the Lagrangian can now be described algebraically:\n",
    "\n",
    "    $ \\frac{\\partial \\mathcal{L}}{\\partial u_k} = \\mu_k - \\mu_{k-1} - l_k c_k + l_k u_k = 0$\n",
    "\n",
    "    Observe that if $c_k \\geq c_{k+1}$ then $u_k = u_{k+1}$. To see this suppose otherwise and assume $u_k < u_{k+1}$ since the other ordering is not optimal. then this implies that\n",
    "    \n",
    "    $c_k + \\frac{1}{l_k}\\bigg(\\mu_{k-1} - \\mu_k \\bigg) < c_{k+1} + \\frac{1}{l_{k+1}}\\bigg(\\mu_{k} - \\mu_{k+1} \\bigg)$\n",
    "    \n",
    "    which implies that $\\mu_k>0$ but we know from the stationary condition for the Lagrangian that $\\mu_k(u_k -u_{k+1}) = 0$ which then gives that either the ordering was assumed in err or $\\mu_k = 0$. Either case contradiction yields the result.\n",
    "       \n",
    "5. So now the coordinates of each row of $X - \\alpha \\nabla F(X)$ may be sorted at each iteration, and the above terms may be evaluated, if $c_k \\geq c_{k+1}$ then we must merge the tuples $\\{(x_k,l_k),(x_{k+1},l_{k+1})\\}$. When we arrive at an ordering such that $c_1 < c_2<\\dots $ then we know that we have found a minimum of the proximity operator minimization problem. This list is then the updated control at a given Armijo backtracking iteration.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10, 300, 1000), (10, 300, 1))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "from scipy.io import loadmat\n",
    "\n",
    "Data = loadmat('hw4_data.mat')\n",
    "A_temp = Data['A']\n",
    "b_temp = Data['b']\n",
    "m = len(A_temp)\n",
    "n, k =  A_temp[0][0].shape\n",
    "\n",
    "A = []\n",
    "b = []\n",
    "for i in range(len(A_temp)):\n",
    "    A.append(A_temp[i][0])\n",
    "    b.append(b_temp[i][0])\n",
    "    \n",
    "A = np.array(A)\n",
    "b = np.array(b)\n",
    "\n",
    "print (A.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16298.048288201997"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class sparse_vector_recovery:\n",
    "    def __init__(self,A,b,lmbda=1e-4):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.m, self.n,self.k = self.A.shape\n",
    "        self.x = np.zeros((self.m,self.k,1))\n",
    "        self.p = None\n",
    "        self.lmbda = lmbda\n",
    "    def evaluate(self, x = None):\n",
    "        if x is None:\n",
    "            x = self.x\n",
    "        cost = 0.0\n",
    "        for i in range(self.m):\n",
    "            Ax_bi = np.dot(A[i],x[i]) - b[i]\n",
    "            cost += np.vdot(Ax_bi,Ax_bi)\n",
    "        return cost\n",
    "    \n",
    "    def weightedsum(self,xklk,xjlj):\n",
    "        (xk,lk) = xklk\n",
    "        (xj,lj) = xjlj\n",
    "        return [(lk*xk+lj*xj)/(lk+lj),(lk+lj)]\n",
    "\n",
    "    def C(self,x_l,k):\n",
    "        assert(k<len(x_l))\n",
    "        x = np.array([x_temp for [x_temp,l_temp] in x_l])\n",
    "        l = np.array([l_temp for [x_temp,l_temp] in x_l])\n",
    "        Ck = x[k]\n",
    "        Ck+=self.lmbda*(sum(l[k+1:])-sum(l[0:k]))\n",
    "        return Ck\n",
    "\n",
    "    def prox(self,z):\n",
    "        for i in range(z.shape[1]):\n",
    "            x =  np.copy(z[:,i,0])\n",
    "            \n",
    "            #sort and store original indices\n",
    "            sortedx=np.sort(x)\n",
    "            originalxmapping=np.argsort(x)\n",
    "\n",
    "            # get unique x's and counts\n",
    "            uniquexs,counts = np.unique(x, return_counts=True)\n",
    "            x_l = [[x,y] for (x,y) in zip(uniquexs,counts)]\n",
    "        \n",
    "            #contract\n",
    "            contractedX_l = []\n",
    "            k = 0\n",
    "            while k < len(x_l)-1:\n",
    "                if (self.C(x_l,k)>=self.C(x_l,k+1)):\n",
    "                    x_l[k+1] = self.weightedsum(x_l[k],x_l[k+1])\n",
    "                    if ((k+1)==len(x_l)-1):\n",
    "                        contractedX_l.append(x_l[k+1])\n",
    "                else:\n",
    "                    contractedX_l.append(x_l[k])\n",
    "                    if ((k+1)==len(x_l)-1):\n",
    "                        contractedX_l.append(x_l[k+1])\n",
    "                k+=1\n",
    "\n",
    "            #[(C_i,l) for i in ..]\n",
    "            proximalXsorted = [[self.C(contractedX_l,i),contractedX_l[i][1]] for i in range(len(contractedX_l))]\n",
    "\n",
    "            #flatten\n",
    "            flatproximalXsorted = []\n",
    "            for (proximalX,num) in proximalXsorted:\n",
    "                for i in range(num):\n",
    "                    flatproximalXsorted.append(proximalX)\n",
    "\n",
    "            proximalX = np.array([x for _,x in sorted(zip(originalxmapping,flatproximalXsorted))])\n",
    "\n",
    "            #store proximal X in the data array\n",
    "            z[:,i,0]=proximalX\n",
    "        return z\n",
    "\n",
    "    def gradient(self,x = None):\n",
    "        if x is None:\n",
    "            x = self.x\n",
    "        grad = np.zeros_like(x)\n",
    "        for i in range(self.m):\n",
    "            Ax_bi = np.dot(A[i],x[i]) - b[i]\n",
    "            grad[i] += 2*np.dot(self.A[i].T,Ax_bi)\n",
    "        return grad\n",
    "    \n",
    "    def grad_norm(self,x = None):\n",
    "        if x is None:\n",
    "            x = self.x\n",
    "        return la.norm(self.gradient(x))\n",
    "    \n",
    "    \n",
    "handler = sparse_vector_recovery(A,b) \n",
    "\n",
    "handler.grad_norm()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration  cost            ||g||L2         alpha          \n",
      "\n",
      "1          6.599190e+04    6.427157e+03    3.906250e-03   \n",
      "\n",
      "2          1.493264e+04    2.774220e+03    3.906250e-03   \n",
      "\n",
      "3          5.139530e+03    1.355570e+03    3.906250e-03   \n",
      "\n",
      "4          2.398500e+03    7.429198e+02    3.906250e-03   \n",
      "\n",
      "5          1.331183e+03    4.599147e+02    3.906250e-03   \n",
      "\n",
      "6          8.821832e+02    6.162923e+02    7.812500e-03   \n",
      "\n",
      "7          4.080506e+02    3.406987e+02    3.906250e-03   \n",
      "\n",
      "8          3.558171e+02    2.907067e+02    4.882812e-04   \n",
      "\n",
      "9          2.082665e+02    1.819972e+02    3.906250e-03   \n",
      "\n",
      "10         1.619256e+02    2.694211e+02    7.812500e-03   \n",
      "\n",
      "11         7.701721e+01    1.520726e+02    3.906250e-03   \n",
      "\n",
      "12         4.611307e+01    7.025228e+01    1.953125e-03   \n",
      "\n",
      "13         3.131216e+01    5.443445e+01    3.906250e-03   \n",
      "\n",
      "14         1.322341e+01    8.796281e+01    1.562500e-02   \n",
      "\n",
      "15         3.715043e+00    2.564158e+01    1.953125e-03   \n",
      "\n",
      "16         2.436413e+00    1.716729e+01    3.906250e-03   \n",
      "\n",
      "17         1.686784e+00    1.273082e+01    3.906250e-03   \n",
      "\n",
      "18         1.364591e+00    2.986476e+01    1.562500e-02   \n",
      "\n",
      "19         2.808591e-01    7.934062e+00    1.953125e-03   \n",
      "\n",
      "20         1.939701e-01    4.113102e+00    1.953125e-03   \n",
      "\n",
      "21         1.164861e-01    8.241966e+00    1.562500e-02   \n",
      "\n",
      "22         4.800951e-02    4.528923e+00    3.906250e-03   \n",
      "\n",
      "23         2.469059e-02    2.599775e+00    3.906250e-03   \n",
      "\n",
      "24         1.508452e-02    1.600618e+00    3.906250e-03   \n",
      "\n",
      "25         1.020130e-02    1.087622e+00    3.906250e-03   \n",
      "\n",
      "26         7.552428e-03    1.567035e+00    7.812500e-03   \n",
      "\n",
      "27         4.413066e-03    9.475352e-01    3.906250e-03   \n",
      "\n",
      "28         2.904518e-03    6.213640e-01    3.906250e-03   \n",
      "\n",
      "29         2.444958e-03    9.767672e-01    7.812500e-03   \n",
      "\n",
      "30         1.340119e-03    5.787352e-01    3.906250e-03   \n",
      "\n",
      "31         8.483946e-04    3.672017e-01    3.906250e-03   \n",
      "\n",
      "32         8.381775e-04    6.227606e-01    7.812500e-03   \n",
      "\n",
      "33         4.259803e-04    3.637123e-01    3.906250e-03   \n",
      "\n",
      "34         2.549920e-04    2.232778e-01    3.906250e-03   \n",
      "\n",
      "35         1.702078e-04    1.485631e-01    3.906250e-03   \n",
      "\n",
      "36         1.422742e-04    2.330832e-01    7.812500e-03   \n",
      "\n",
      "37         7.947276e-05    1.396825e-01    3.906250e-03   \n",
      "\n",
      "38         5.077981e-05    8.913902e-02    3.906250e-03   \n",
      "\n",
      "39         5.022618e-05    1.518858e-01    7.812500e-03   \n",
      "\n",
      "40         2.580859e-05    8.916343e-02    3.906250e-03   \n",
      "\n",
      "41         1.558100e-05    5.507601e-02    3.906250e-03   \n",
      "\n",
      "42         1.044417e-05    3.667677e-02    3.906250e-03   \n",
      "\n",
      "43         8.803500e-06    5.795308e-02    7.812500e-03   \n",
      "\n",
      "44         4.939313e-06    3.479102e-02    3.906250e-03   \n",
      "\n",
      "45         3.170932e-06    2.227737e-02    3.906250e-03   \n",
      "\n",
      "46         3.158378e-06    3.808945e-02    7.812500e-03   \n",
      "\n",
      "47         1.631450e-06    2.243844e-02    3.906250e-03   \n",
      "\n",
      "48         9.870969e-07    1.385819e-02    3.906250e-03   \n",
      "\n",
      "49         6.639955e-07    9.247912e-03    3.906250e-03   \n",
      "\n",
      "50         5.636941e-07    1.466771e-02    7.812500e-03   \n",
      "\n",
      "51         3.174632e-07    8.828054e-03    3.906250e-03   \n",
      "\n",
      "52         2.041658e-07    5.646993e-03    3.906250e-03   \n",
      "\n",
      "53         1.421245e-07    3.940317e-03    3.906250e-03   \n",
      "\n",
      "54         1.060139e-07    5.717560e-03    7.812500e-03   \n",
      "\n",
      "55         6.436311e-08    3.538921e-03    3.906250e-03   \n",
      "\n",
      "56         4.337565e-08    2.358753e-03    3.906250e-03   \n",
      "\n",
      "57         3.703942e-08    3.758576e-03    7.812500e-03   \n",
      "\n",
      "58         2.090265e-08    2.262018e-03    3.906250e-03   \n",
      "\n",
      "59         1.348771e-08    1.450034e-03    3.906250e-03   \n",
      "\n",
      "60         9.407295e-09    1.010803e-03    3.906250e-03   \n",
      "\n",
      "61         7.050072e-09    1.472697e-03    7.812500e-03   \n",
      "\n",
      "62         4.289797e-09    9.113897e-04    3.906250e-03   \n",
      "\n",
      "63         2.900111e-09    6.088356e-04    3.906250e-03   \n",
      "\n",
      "64         2.480406e-09    9.706481e-04    7.812500e-03   \n",
      "\n",
      "65         1.405817e-09    5.854808e-04    3.906250e-03   \n",
      "\n",
      "66         9.091582e-10    3.752826e-04    3.906250e-03   \n",
      "\n",
      "67         6.359208e-10    2.622286e-04    3.906250e-03   \n",
      "\n",
      "68         4.772593e-10    3.820898e-04    7.812500e-03   \n",
      "\n",
      "69         2.915630e-10    2.370290e-04    3.906250e-03   \n",
      "\n",
      "70         1.975190e-10    1.583500e-04    3.906250e-03   \n",
      "[[[-0.21032977]\n",
      "  [ 0.04540779]\n",
      "  [-0.85449972]\n",
      "  ..., \n",
      "  [ 1.11477339]\n",
      "  [-0.06116396]\n",
      "  [-0.21082977]]\n",
      "\n",
      " [[ 0.42089302]\n",
      "  [-0.07203142]\n",
      "  [ 0.62260611]\n",
      "  ..., \n",
      "  [-1.69855146]\n",
      "  [ 2.01996587]\n",
      "  [ 0.42099302]]\n",
      "\n",
      " [[ 0.45813684]\n",
      "  [-1.19345645]\n",
      "  [-1.2851623 ]\n",
      "  ..., \n",
      "  [ 1.68047019]\n",
      "  [ 0.4524017 ]\n",
      "  [ 0.45843684]]\n",
      "\n",
      " ..., \n",
      " [[-1.96228044]\n",
      "  [-2.46901711]\n",
      "  [ 0.29259276]\n",
      "  ..., \n",
      "  [ 1.30484221]\n",
      "  [ 0.67569727]\n",
      "  [-1.96318044]]\n",
      "\n",
      " [[ 0.28830201]\n",
      "  [ 1.33193525]\n",
      "  [ 0.46278526]\n",
      "  ..., \n",
      "  [ 0.29091898]\n",
      "  [-0.17488965]\n",
      "  [ 0.28800201]]\n",
      "\n",
      " [[ 0.60893449]\n",
      "  [-0.07414766]\n",
      "  [-0.31606207]\n",
      "  ..., \n",
      "  [ 0.33742918]\n",
      "  [-0.23668817]\n",
      "  [ 0.60963449]]]\n"
     ]
    }
   ],
   "source": [
    "def proximal_steepest_descent(handler,A,b,x_0, tolerance = 1e-8,\n",
    "             max_iters = 10000,back_track = False, eta_option =0,\n",
    "             min_step = 1e-10,c_armijo = 1e-4,verbose = False):\n",
    "    f = handler(A,b)\n",
    "    f.p = np.zeros_like(x_0)\n",
    "    iters = 0\n",
    "    alpha = 1.0\n",
    "    grad_norm0 = f.grad_norm()\n",
    "    grad_norm = grad_norm0\n",
    "    \n",
    "    while grad_norm > tolerance*grad_norm0 and iters <= max_iters:\n",
    "        cost = f.evaluate()\n",
    "        g = f.gradient()\n",
    "        f.p = -g\n",
    "        p_inner_g = np.vdot(g,f.p)\n",
    "        if back_track:\n",
    "            cost_old = cost\n",
    "            alpha = 1.\n",
    "            while alpha > min_step:\n",
    "                temp = f.prox(f.x + alpha*f.p)\n",
    "                cost = f.evaluate(temp)\n",
    "                if cost < cost_old -c_armijo*alpha*p_inner_g:\n",
    "                    break\n",
    "                else:\n",
    "                    alpha *= 0.5\n",
    "        f.x = f.prox(f.x+alpha*f.p)\n",
    "        grad_norm = f.grad_norm()\n",
    "        if verbose:\n",
    "            if iters == 0:\n",
    "                print (\"\\n{0:10} {1:15} {2:15} {3:15}\".format(\n",
    "                      \"Iteration\",  \"cost\", \"||g||L2\", \"alpha\" ))\n",
    "            else:\n",
    "                print (\"\\n{0:<10d} {1:<15e} {2:<15e} {3:<15e}\".format(\n",
    "                      iters,  cost, grad_norm, alpha ))\n",
    "        iters += 1\n",
    "        converged = (grad_norm < tolerance*grad_norm0)\n",
    "    return f.x,iters, converged\n",
    "\n",
    "x_0 = np.ones((m,k,1))\n",
    "\n",
    "x,iters,converged = proximal_steepest_descent(sparse_vector_recovery,A,b,x_0,\\\n",
    "                                     back_track=True, verbose = True)\n",
    "\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
